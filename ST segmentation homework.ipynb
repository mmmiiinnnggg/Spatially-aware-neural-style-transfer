{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSU CMC - spatially aware style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You only need to send your solution, without accompanying files, in uncompressed form\n",
    "* Rename your solution in the format [name]\\_[surname].ipynb, for example Ivan_Ivanov.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, you will modify [the baseline optimization-based style transfer method](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), with the following [implementation](https://github.com/leongatys/PytorchNeuralStyleTransfer/blob/master/NeuralStyleTransfer.ipynb). It makes uniform stylization of the whole content image with the whole style image. However, real artists apply different style to semantically different objects, such as people, buildings, cars and the background. You need to make spatially aware style transfer using several approaches, so that each part of content is stylized with some part of style (style parts may intersect). If you have any questions, write in telegram to @VictorKitov, I will answer to common chat.\n",
    "\n",
    "The content image is split into parts based on semantic segmentation, that works on demo content images (`Images/Contents`). The style image is split into parts using [superpixels](https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_segmentations.html) - you need to try several approaches and finetune hyperparameters for each approach. Afterwards the matching procedure takes place. Similar parts of style are matched to similar parts of content (many to one mapping). You need to select proximity criterion yourself, that works better - this may be closeness in RGB color distributuion or closeness in distribution of activations at some intermediate layer of the VGG neural network (measures closeness in patterns rather than in colors only). Alternatively this may be closeness in color distribution after both images were converted to grayscale (since originally content and style may consist of very different colors). Ð¡loseness may be estimated by comparing the vectors of means and standard deviations for content and style part by Euclidean or cosine distance. \n",
    "\n",
    "Semantically aware style transfer can generate stylizations where different parts of content vary in style too much so an *averaging hyperparameter* is required: if 0, different parts of content are stylized with different parts of style. The closer is this hyperparameter to 1, the more uniform stylization becomes (more mixing of styles from all available superpixels at the matching stage). If 1 exaclty, the method reduces to uniform baseline style transfer.\n",
    "\n",
    "Reporting your solution\n",
    "\n",
    "The solution consists of several approaches that you try. Each approach starts from H1 heading \"Method 1\", \"Method2\", etc. Each method starts from textual description in markdown of your approach (pseudocode with latex preferred for clarity), naming which segmentation and superpixel algorithm with which parameters you choose, how you estimate closeness between segments and superpixels.\n",
    "Then comes the code with its output: for _every content image in `Images/Contents` and every style image from `Images/Styles` (16 pairs)_ you show content  and its segmentation mask (1st row), style image and its superpixel mask (2nd row, matched parts of content and style should have the same color) and baseline stylization and semantically aware stylization side-by-side (3rd row). Each row extends to full jupyter notebook width for better visualization.\n",
    "\n",
    "You solution is judged accorind to the following criteria\n",
    "* Does it coinside with solution of some other student? If yes, both students get 0 points for the solution.\n",
    "* How many approaches you test (+2 points for every method, 6 points max. Methods may differ in superpixel algorithm, preprocessing (color or grayscale) or closeness metric. Using the same method with different hyperparameters does not count - you should adjust hyperparameters for each method and report results with beset hyperparameters only.)\n",
    "* Visual quality of your solutions compared to baseline (how well you selected the hyperparameters) - 4 points.\n",
    "* Implement *averaging hyperparameter* for some method (2 points). You make a heading \"Averaging\", provide a markdown description of the averaging approach, the code and visualize content, style (one pair is enough) and 4 stylizations for averaging parameter 0, 0.33, 0.66, 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
